{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b39da96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\137088\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cca74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=\"Despite the meteorological warnings, the expedition continued into the storm-laden valley.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7224af55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Despite', 'the', 'meteorological', 'warnings', ',', 'the', 'expedition', 'continued', 'into', 'the', 'storm-laden', 'valley', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5856af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2='He whispered, \"Is this... what they call destiny?\" while the orchestra crescendoed dramatically.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa1c006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'whispered', ',', '``', 'Is', 'this', '...', 'what', 'they', 'call', 'destiny', '?', \"''\", 'while', 'the', 'orchestra', 'crescendoed', 'dramatically', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e9a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U.N.', 'officials', 'arrived', 'in', 'the', 'pre-dawn', 'hours', ';', 'negotiations', ',', 'however', ',', 'remained', 'inconclusive', '.']\n"
     ]
    }
   ],
   "source": [
    "s3= \"U.N. officials arrived in the pre-dawn hours; negotiations, however, remained inconclusive.\"\n",
    "print(word_tokenize(s3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1951533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.', \"O'Connor\", \"'s\", 'postdoctoral', 'thesis—though', 'controversial—challenged', 'century-old', 'beliefs', '.']\n"
     ]
    }
   ],
   "source": [
    "s4=\"Dr. O'Connor's postdoctoral thesis—though controversial—challenged century-old beliefs.\"\n",
    "print(word_tokenize(s4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c049ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", 'not', 'just', 'about', '``', 'what', \"''\", 'was', 'said', ',', 'but', 'also', 'how', 'it', 'was', 'delivered—with', 'venom', ',', 'or', 'with', 'vigor']\n"
     ]
    }
   ],
   "source": [
    "s5=  \"\"\"It's not just about \"what\" was said, but also how it was delivered—with venom, or with vigor\"\"\"\n",
    "print(word_tokenize(s5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ed7e9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 1 Sentences:\n",
      "- Global warming, once considered a distant threat, is now causing visible shifts in climate.\n",
      "- Droughts, floods, and rising sea levels are no longer anomalies.\n",
      "- Are we ready to face the consequences?\n",
      "- Scientists urge immediate action.\n"
     ]
    }
   ],
   "source": [
    "## 1b. sentence tokenization\n",
    "paragraph1 = \"\"\"Global warming, once considered a distant threat, is now causing visible shifts in climate. \n",
    "Droughts, floods, and rising sea levels are no longer anomalies. \n",
    "Are we ready to face the consequences? Scientists urge immediate action.\"\"\"\n",
    "\n",
    "\n",
    "sentences1 = sent_tokenize(paragraph1)\n",
    "print(\"Paragraph 1 Sentences:\")\n",
    "for sentence in sentences1:\n",
    "    print(\"-\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcb80087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paragraph 2 Sentences:\n",
      "- \"Tell me the truth,\" she demanded.\n",
      "- Silence.\n",
      "- The room felt heavier with every second that passed.\n",
      "- Eventually, he sighed, \"I can’t... not now.\"\n"
     ]
    }
   ],
   "source": [
    "paragraph2 = '''\"Tell me the truth,\" she demanded. Silence. The room felt heavier with every second that passed. Eventually, he sighed, \"I can’t... not now.\"'''\n",
    "\n",
    "sentences2 = sent_tokenize(paragraph2)\n",
    "\n",
    "print(\"\\nParagraph 2 Sentences:\")\n",
    "for sentence in sentences2:\n",
    "    print(\"-\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c646e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paragraph 3 Sentences:\n",
      "- The CEO, after a brief pause, addressed the shareholders.\n",
      "- \"Ladies and gentlemen,\" he began, \"the future of this company lies in innovation.\"\n",
      "- The audience erupted in applause.\n"
     ]
    }
   ],
   "source": [
    "paragraph3 = '''The CEO, after a brief pause, addressed the shareholders. \"Ladies and gentlemen,\" he began, \"the future of this company lies in innovation.\" The audience erupted in applause.'''\n",
    "\n",
    "\n",
    "sentences3 = sent_tokenize(paragraph3)\n",
    "\n",
    "print(\"\\nParagraph 3 Sentences:\")\n",
    "for sentence in sentences3:\n",
    "    print(\"-\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fb2c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\137088\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "## 2A. Stop words removal\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa27bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 (No Stopwords): ['realm', 'artificial', 'intelligence', 'decisions', 'learned', 'refined', 'optimized']\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"In the realm of artificial intelligence, decisions are not just made—they're learned, refined, and optimized.\"\n",
    "tokens1 = word_tokenize(sentence1)\n",
    "filtered1 = [word for word in tokens1 if word.lower() not in stop_words and word.isalpha()]\n",
    "print(\"Sentence 1 (No Stopwords):\", filtered1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e1dd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 2 (No Stopwords): ['Though', 'tried', 'answer', 'never', 'came', 'easily', 'hidden', 'behind', 'layers', 'ambiguity']\n"
     ]
    }
   ],
   "source": [
    "sentence2 = \"Though she tried, the answer never came to her easily; it was hidden behind layers of ambiguity.\"\n",
    "tokens2 = word_tokenize(sentence2)\n",
    "filtered2 = [word for word in tokens2 if word.lower() not in stop_words and word.isalpha()]\n",
    "print(\"Sentence 2 (No Stopwords):\", filtered2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8010139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 3 (No Stopwords): ['Without', 'doubt', 'process', 'tedious', 'yet', 'results', 'proved', 'worth', 'struggle']\n"
     ]
    }
   ],
   "source": [
    "sentence3 = \"Without a doubt, the process was tedious, yet the results proved to be worth the struggle.\"\n",
    "tokens3 = word_tokenize(sentence3)\n",
    "filtered3 = [word for word in tokens3 if word.lower() not in stop_words and word.isalpha()]\n",
    "print(\"Sentence 3 (No Stopwords):\", filtered3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ab5512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 4 (No Stopwords): ['neither', 'mentally', 'adrift', 'sea', 'uncertainty']\n"
     ]
    }
   ],
   "source": [
    "sentence4 = \"He was neither here nor there, mentally adrift in a sea of uncertainty.\"\n",
    "tokens4 = word_tokenize(sentence4)\n",
    "filtered4 = [word for word in tokens4 if word.lower() not in stop_words and word.isalpha()]\n",
    "print(\"Sentence 4 (No Stopwords):\", filtered4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bb10a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbcec540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 - Stemmed Words: ['argu', 'aggress', 'inconclus', 'result', 'emerg']\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "words1 = [\"arguing\", \"aggressively\", \"inconclusive\", \"results\", \"emerged\"]  ## words to be stemmed\n",
    "stems1 = [stemmer.stem(word) for word in words1]\n",
    "print(\"Sentence 1 - Stemmed Words:\", stems1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0c8834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 2 - Stemmed Words: ['connect', 'meaning', 'dissolv']\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "words2 = [\"connections\", \"meaningful\", \"dissolving\"]\n",
    "stems2 = [stemmer.stem(word) for word in words2]\n",
    "print(\"Sentence 2 - Stemmed Words:\", stems2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a76977db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 3 - Stemmed Words: ['manag', 'delay', 'implement', 'financi', 'constraint']\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "words3 = [\"management\", \"delaying\", \"implementation\", \"financial\", \"constraints\"]\n",
    "stems3 = [stemmer.stem(word) for word in words3]\n",
    "print(\"Sentence 3 - Stemmed Words:\", stems3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e03ad3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 4 - Stemmed Words: ['hope', 'pray', 'plead']\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "words4 = [\"hoped\", \"prayed\", \"pleaded\"]\n",
    "stems4 = [stemmer.stem(word) for word in words4]\n",
    "print(\"Sentence 4 - Stemmed Words:\", stems4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
